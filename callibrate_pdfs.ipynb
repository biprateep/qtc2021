{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNhx0u1vszZN"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpl_patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "mpl.rc(\"text\", usetex=False)\n",
    "\n",
    "import cdetools\n",
    "import corner\n",
    "import flexcode\n",
    "import qp\n",
    "import rail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZsmK6L8v9OO"
   },
   "source": [
    "### Estimating photo-$z$ posterior PDFs with FlexCode\n",
    "\n",
    "A demonstration of `FlexCode` in the context of photo-$z$s can be found in [Dalmasso, et al 2019](https://arxiv.org/abs/1908.11523), with demos in `R` published on [GitHub](https://github.com/Mr8ND/cdetools_applications).\n",
    "We'll demonstrate it on the pzflow-based samples generated from the Happy/Teddy data sets.*italicized text*\n",
    "\n",
    "_This content is adapted from FlexCode's [Teddy tutorial](https://github.com/tpospisi/FlexCode/blob/master/tutorial/Flexcode-tutorial-teddy.ipynb), written by Nic Dalmasso (CMU)._\n",
    "_The same functionality can also be accessed through a `rail.Estimator()` object; see [tutorial](https://github.com/LSSTDESC/RAIL/blob/master/examples/estimation/RAIL_estimation_demo.ipynb) written by Sam Schmidt (UC Davis)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGZ8i_opZiFY"
   },
   "outputs": [],
   "source": [
    "z_min, z_max = 0.0, 1.5\n",
    "r_min, r_max = 10.0, 25.0\n",
    "granularity = 100\n",
    "grid = np.linspace(z_min, z_max, granularity)\n",
    "\n",
    "n_grid = granularity\n",
    "\n",
    "# Select regression method\n",
    "from flexcode.regression_models import NN\n",
    "\n",
    "# Parameters\n",
    "basis_system = \"cosine\"  # Basis system\n",
    "max_basis = 31  # Maximum number of basis. If the model is not tuned,\n",
    "# max_basis is set as number of basis\n",
    "\n",
    "# Regression Parameters\n",
    "# If a list is passed for any parameter automatic 5-fold CV is used to\n",
    "# determine the best parameter combination.\n",
    "params = {\n",
    "    \"k\": 5\n",
    "}  # [5, 10, 15, 20]}       # A dictionary with method-specific regression parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udbXVbiDZwie"
   },
   "source": [
    "Read Data set and divide into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb0yIUIjXSJm"
   },
   "outputs": [],
   "source": [
    "x_orig = pd.read_csv(\"../photoz_catalogues/test_set_photometry.csv\")[\n",
    "    [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\"]\n",
    "].to_numpy()\n",
    "y_orig = pd.read_csv(\"../photoz_catalogues/test_set_redshifts.csv\")[\n",
    "    [\"redshift\"]\n",
    "].to_numpy()\n",
    "posteriors_orig = pd.DataFrame(\n",
    "    np.load(\"../photoz_catalogues/test_set_posteriors.csv\")\n",
    ").to_numpy()\n",
    "\n",
    "# n_samp = 10000\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test, posteriors_train, posteriors_test = train_test_split(\n",
    "    x_orig, y_orig, posteriors_orig, train_size=2000, random_state=42\n",
    ")\n",
    "\n",
    "(\n",
    "    x_train,\n",
    "    x_validation,\n",
    "    y_train,\n",
    "    y_validation,\n",
    "    posteriors_train,\n",
    "    posteriors_validation,\n",
    ") = train_test_split(\n",
    "    x_train, y_train, posteriors_train, train_size=1000, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lo6J8wY7pb1U"
   },
   "outputs": [],
   "source": [
    "# Parameterize model\n",
    "model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
    "\n",
    "# Fit model - this will also choose the optimal number of neighbors `k`\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Tune model - Select the best number of basis\n",
    "model.tune(x_validation, y_validation)\n",
    "\n",
    "# Predict new densities on grid\n",
    "cde_test, y_grid = model.predict(x_test, n_grid=n_grid)\n",
    "\n",
    "cde_validation, y_grid = model.predict(x_validation, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine one of the photo-$z$ posteriors estimated with the perfectly representative training/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "sQ8plrbNCoLn",
    "outputId": "7505e7b5-bccc-403c-8d70-f37ccd0e2794"
   },
   "outputs": [],
   "source": [
    "chosen = 9\n",
    "\n",
    "plt.plot(y_grid, cde_test[chosen], label=\"Estimated posterior\")\n",
    "plt.plot(grid, posteriors_test[chosen], label=\"True posterior\")\n",
    "plt.axvline(y_test[chosen], 0, 1, c=\"C3\", label=\"True redshift\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAF62sfOawmd"
   },
   "source": [
    "It looks pretty good!\n",
    "Now let's try training and validating with some Happy/Teddy data but estimating posteriors on the test set from the pzflow demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWljb73yRRlT"
   },
   "outputs": [],
   "source": [
    "# y_train = pd.read_csv(\"teddyAredshifts.csv\")[\"redshift\"].to_numpy()\n",
    "# x_train = pd.read_csv(\"teddyAphotometry.csv\")[\n",
    "#     [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\"]\n",
    "# ].to_numpy()\n",
    "\n",
    "# y_validation = pd.read_csv(\"teddyBredshifts.csv\")[\"redshift\"].to_numpy()\n",
    "# x_validation = pd.read_csv(\"teddyBphotometry.csv\")[\n",
    "#     [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\"]\n",
    "# ].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmNhNgiIZp12"
   },
   "outputs": [],
   "source": [
    "# # Parameterize model\n",
    "# model = flexcode.FlexCodeModel(NN, max_basis, basis_system, regression_params=params)\n",
    "\n",
    "# # Fit model - this will also choose the optimal number of neighbors `k`\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# # # # Tune model - Select the best number of basis\n",
    "# model.tune(x_validation, y_validation)\n",
    "\n",
    "# # # Predict new densities on grid\n",
    "# cde_test_bias, y_grid_bias = model.predict(x_test, n_grid=n_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwnYgZL6aWUP"
   },
   "outputs": [],
   "source": [
    "# plt.plot(\n",
    "#     y_grid_bias,\n",
    "#     cde_test_bias[chosen],\n",
    "#     label=\"Estimated posterior (biased training set)\",\n",
    "# )\n",
    "# plt.plot(y_grid, cde_test[chosen], label=\"Estimated posterior (unbiased training set)\")\n",
    "# plt.plot(grid, posteriors_test[chosen], label=\"True posterior\")\n",
    "# plt.axvline(y_test[chosen], 0, 1, c=\"C3\", label=\"True redshift\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"redshift\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Correcting the PDFs using local regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pit(cdes, z_grid, z_test):\n",
    "    \"\"\"\n",
    "    Calculates coverage based upon the CDF\n",
    "\n",
    "    @param cdes: a numpy array of conditional density estimates;\n",
    "        each row corresponds to an observation, each column corresponds to a grid\n",
    "        point\n",
    "    @param z_grid: a numpy array of the grid points at which cde_estimates is evaluated\n",
    "    @param z_test: a numpy array of the true z values corresponding to the rows of cde_estimates\n",
    "\n",
    "    @returns A numpy array of values\n",
    "    \"\"\"\n",
    "\n",
    "    nrow_cde, ncol_cde = cdes.shape\n",
    "    n_samples = z_test.shape[0]\n",
    "    n_grid_points = z_grid.shape[0]\n",
    "\n",
    "    if nrow_cde != n_samples:\n",
    "        raise ValueError(\n",
    "            \"Number of samples in CDEs should be the same as in z_test.\"\n",
    "            \"Currently %s and %s.\" % (nrow_cde, n_samples)\n",
    "        )\n",
    "    if ncol_cde != n_grid_points:\n",
    "        raise ValueError(\n",
    "            \"Number of grid points in CDEs should be the same as in z_grid.\"\n",
    "            \"Currently %s and %s.\" % (nrow_cde, n_grid_points)\n",
    "        )\n",
    "\n",
    "    z_min = np.min(z_grid)\n",
    "    z_max = np.max(z_grid)\n",
    "    z_delta = (z_max - z_min) / (n_grid_points - 1)\n",
    "    # This line can be vectorized\n",
    "    vals = [\n",
    "        z_delta * np.sum(cdes[ii, np.where(z_grid < z_test[ii])[0]])\n",
    "        for ii in range(n_samples)\n",
    "    ]\n",
    "    return np.array(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_values_test = get_pit(\n",
    "    cde_test, np.squeeze(y_grid, axis=-1), np.squeeze(y_test, axis=-1)\n",
    ")\n",
    "pit_values_validation = get_pit(\n",
    "    cde_validation, np.squeeze(y_grid, axis=-1), np.squeeze(y_validation)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Global PIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good model\n",
    "from plot_utils import plot_with_uniform_band\n",
    "\n",
    "fig_pit_good = plot_with_uniform_band(\n",
    "    values=pit_values_test,\n",
    "    ci_level=0.95,\n",
    "    x_label=\"PIT values\",\n",
    "    n_bins=15,\n",
    "    ylim=[0, 400],\n",
    ")\n",
    "fig_pit_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Global PIT correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "The PIT is defined by the transformation \n",
    "\n",
    "$$PIT(z) = \\int_{0}^{z} f(z'|...)dz'$$\n",
    "\n",
    "where, $f(z'|...)$ is the conditional density estimate produced by the algorithm. \n",
    "If my estimated PDFS are right, then for a given population the true value should lie within the the $q$th quantile for $q$ fraction of times. \n",
    "\n",
    "Mathematically, we define the new random variable $P=PIT(z_{true})$. This means the probability distribution for the random variable P, fiven by $F(p)$ should follow a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.linspace(0,1,101)\n",
    "num_bins = 100\n",
    "pops, bins = np.histogram(pit_values_validation, bins=num_bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction = np.zeros((len(x_test), n_grid))\n",
    "for i in range(len(x_test)):\n",
    "    correction[i] = get_pit(np.tile(cde_test[i],(n_grid,1)), np.squeeze(y_grid), np.squeeze(y_grid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_idx = (np.digitize(correction, bins) - 1)\n",
    "bin_idx[bin_idx==num_bins]=(num_bins-1)\n",
    "\n",
    "correction = pops[bin_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_test_correct = cde_test*correction\n",
    "cde_test_correct = cde_test_correct/(np.gradient(np.squeeze(y_grid))*np.sum(cde_test_correct, axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = 9\n",
    "\n",
    "plt.plot(y_grid, cde_test_correct[chosen], label=\"Estimated posterior\")\n",
    "plt.plot(grid, posteriors_test[chosen], label=\"True posterior\")\n",
    "plt.axvline(y_test[chosen], 0, 1, c=\"C3\", label=\"True redshift\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_values_test_correct = get_pit(\n",
    "    cde_test_correct, np.squeeze(y_grid, axis=-1), np.squeeze(y_test, axis=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pit_good = plot_with_uniform_band(\n",
    "    values=pit_values_test_correct,\n",
    "    ci_level=0.95,\n",
    "    x_label=\"PIT values\",\n",
    "    n_bins=15,\n",
    "    ylim=[0, 400],\n",
    ")\n",
    "fig_pit_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show that global callibration does not mean local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cde_diagnostics.local_histogram import local_histogram\n",
    "\n",
    "fig = local_histogram(x_train=x_test, pit_train=pit_values_test,\n",
    "                      x_test=[(0,0,0,0,0)],\n",
    "                      alphas=np.linspace(0.0, 0.99, 99), clf_name='XGBoost \\n (d3, n1000)',\n",
    "                      ci_level=0.05, n_bins=7, figsize=(5,4)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps:\n",
    "- for each data point in the test set predict the local PIT based on the dev set, have to do it fast (XGB+GPU?)\n",
    "- following that use the same procedure as above to find the correction term.\n",
    "- because of the binned nature of the correction the PDFs become spikey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg4SzDIYiTGf"
   },
   "source": [
    "## 3. Evaluating the performance of estimated photo-$z$ posterior PDFs\n",
    "\n",
    "Once we have estimated photo-$z$ posterior PDFs, we need a way to determine if they're actually any good.\n",
    "Since the tutorial has only one method but multiple training/validation sets, that's all we can compare for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYd_evyhw3pC"
   },
   "source": [
    "### Metrics of estimated photo-$z$ posteriors and true redshifts\n",
    "\n",
    "First, let's try out a couple metrics of estimated photo-$z$ posteriors that do not require knowledge of the true photo-$z$ posteriors.\n",
    "There's additional functionality for the case of having true redshifts but not true posteriors in [cdetools](https://github.com/tpospisi/cdetools) and [cde-diagnostics](https://github.com/zhao-david/CDE-diagnostics), but this should give a general idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L53A-iBubIvZ"
   },
   "outputs": [],
   "source": [
    "from cdetools import cde_loss, cdf_coverage, hpd_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sVDo_DLgGit"
   },
   "source": [
    "The Probability Integral Transform (PIT) is defined as \n",
    "\\begin{equation}\n",
    "PIT = \\int_{-\\infty}^{z_{true}} p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) dz .\n",
    "\\end{equation}\n",
    "A histogram of PIT values is commonly used to assess how consistent a population of photo-$z$ PDFs are with the true redshifts.\n",
    "Ideally, it would be a uniform distribution, meaning N% of galaxies have their true redshift within the Nth percentile of their estimated photo-$z$ posterior PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "uCWuAkGWk2ik",
    "outputId": "084df993-edb1-48f2-c743-5f4514e30217"
   },
   "outputs": [],
   "source": [
    "pit_values = cdf_coverage.cdf_coverage(cde_test, y_grid, y_test)\n",
    "pit_values_bias = cdf_coverage.cdf_coverage(cde_test_bias, y_grid_bias, y_test)\n",
    "\n",
    "plt.hist(pit_values, alpha=0.5, bins=100, label=\"representative\")\n",
    "plt.hist(pit_values_bias, alpha=0.5, bins=100, label=\"biased\")\n",
    "plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.xlabel(\"PIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKxtSTGtlrbq"
   },
   "source": [
    "The Highest Predictive Density (HPD) \n",
    "\\begin{equation}\n",
    "HPD = \\int_{z': p(z' | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) \\geq p(z | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)} p(z' | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi) dz\n",
    "\\end{equation}\n",
    "is like the area of the PDF where it exceeds a given value.\n",
    "Over a population, it would ideally be flat, like the PIT.\n",
    "[A talk by David Zhao (CMU)](https://drive.google.com/file/d/1uvPtK_RcTUHEwt0ZYld41VKEPHnehWbN/view) has a lovely visualization of the HPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84WUFQ8rfVtZ"
   },
   "outputs": [],
   "source": [
    "hpd_cov = hpd_coverage.hpd_coverage(cde_test, y_grid, y_test)\n",
    "hpd_cov_bias = hpd_coverage.hpd_coverage(cde_test_bias, y_grid_bias, y_test)\n",
    "\n",
    "plt.hist(hpd_cov, alpha=0.5, bins=100, label=\"representative\")\n",
    "plt.hist(hpd_cov_bias, alpha=0.5, bins=100, label=\"biased\")\n",
    "plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.xlabel(\"HPD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgkvMM8enL48"
   },
   "source": [
    "The CDE loss \n",
    "\\begin{equation}\n",
    "\\hat{L} = \\frac{1}{K} \\sum_{i=1}^{K} \\int \\left(p(z | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)\\right)^{2} dz - \\frac{2}{K} \\sum_{i=1}^{K} p(z_{i} | \\vec{d}_{i}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)\n",
    "\\end{equation}\n",
    "approximates the true posterior from the estimated posterior evaluated at the true redshift.\n",
    "It's explained quite well in [a talk by Nic Dalmasso (CMU)](https://www.dropbox.com/s/2r4tl4qv0iyqo9b/STAMPS_LSST_CDE_Tools_Presentation.pdf?dl=0).\n",
    "A lower value indicates a better estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQq6sv7JV4bH",
    "outputId": "3f0d78c8-9e80-4cc9-c353-ff96db968392"
   },
   "outputs": [],
   "source": [
    "print(cde_loss.cde_loss(cde_test, y_grid, y_test))\n",
    "print(cde_loss.cde_loss(cde_test_bias, y_grid_bias, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbTHU-y6xNKX"
   },
   "source": [
    "### Comparison of estimated and true photo-$z$ posterior PDFs\n",
    "\n",
    "There are two categories of metrics of approximated and true PDF: \n",
    "those that either rely upon or force the normalization condition $\\int p(z) dz = 1$and those that evaluate differences between arbitrary functions.\n",
    "`qp` [(Malz, et al 2018)](https://arxiv.org/abs/1806.00014) is a package for manipulating univariate PDFs under many parameterizations and includes a few comparison metrics.\n",
    "\n",
    "The [original version](https://github.com/aimalz/qp) consistently enforced normalization but had limited functionality, whereas the [new version](https://github.com/LSSTDESC/qp) includes many more parameterizations whose usage is \"at your own risk\" in terms of possibly violating normalization.\n",
    "We'll use the new version for the sake of speed but evaluating metrics using simplified functions ported from the old version due to a (hopefully transient) bug in the handling of large sets of posteriors.\n",
    "The first step is to get both the true posteriors and the approximations evaluated on the same grid of redshifts.\n",
    "\n",
    "_This content is adapted from the [qp demo](https://github.com/LSSTDESC/qp/blob/master/docs/notebooks/demo.ipynb), written by Alex Malz (GCCL@RUB), Phil Marshall (SLAC), and Eric Charles (SLAC), and [qp metrics demo](https://github.com/LSSTDESC/qp/blob/master/docs/notebooks/kld.ipynb), written by Alex Malz (GCCL@RUB) and Phil Marshall (SLAC)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WARGQ6Gh-k_"
   },
   "outputs": [],
   "source": [
    "# P = qp.Ensemble(qp.interp, data=dict(xvals=grid.reshape(grid.shape[0]), yvals=posteriors_test))\n",
    "Q = qp.Ensemble(\n",
    "    qp.interp, data=dict(xvals=y_grid.reshape(y_grid.shape[0]), yvals=cde_test)\n",
    ")\n",
    "Q_bias = qp.Ensemble(\n",
    "    qp.interp,\n",
    "    data=dict(xvals=y_grid.reshape(y_grid_bias.shape[0]), yvals=cde_test_bias),\n",
    ")\n",
    "grid, approx_pdf_on_grid = Q.gridded(grid)\n",
    "grid, approx_pdf_on_grid_bias = Q_bias.gridded(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioNxxbaDoDEm"
   },
   "source": [
    "The Kullback Leibler Divergence (KLD)\n",
    "\\begin{equation}\n",
    "KLD = \\int_{-\\infty}^{\\infty} p(z | \\vec{d}) \\log\\left[\\frac{p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)}{p(z | \\vec{d})}\\right] dz\n",
    "\\end{equation}\n",
    "is a directional measure of how much information is lost by using the estimated $p(z | \\vec{d}, \\{z_{n}, \\vec{d}_{n}\\}_{N}, \\pi)$ instead of the true $p(z | \\vec{d})$.\n",
    "We want the KLD for each galaxy to be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "J-J7YdTtLKPf",
    "outputId": "6d4c71d2-ec4c-4e68-b075-5b3ccd8382db"
   },
   "outputs": [],
   "source": [
    "KLDs = np.array(\n",
    "    [\n",
    "        qp.metrics.quick_kld(p, q, dx=np.mean(grid[1:] - grid[:-1]))\n",
    "        for p, q in zip(posteriors_test, approx_pdf_on_grid)\n",
    "    ]\n",
    ")\n",
    "KLDs_bias = np.array(\n",
    "    [\n",
    "        qp.metrics.quick_kld(p, q, dx=np.mean(grid[1:] - grid[:-1]))\n",
    "        for p, q in zip(posteriors_test, approx_pdf_on_grid_bias)\n",
    "    ]\n",
    ")\n",
    "\n",
    "plt.hist(np.log(KLDs), alpha=0.5, bins=100, label=\"representative\", density=True)\n",
    "plt.hist(np.log(KLDs_bias), alpha=0.5, bins=100, label=\"biased\", density=True)\n",
    "plt.xlabel(\"KLD\")\n",
    "plt.legend()\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR96wyRGPerk"
   },
   "source": [
    "The root-mean-square-error (RMSE) is a symmetric measure commonly used to compare 1D functions.\n",
    "**TODO: write it out?** Similarly, a lower value corresponds to a more closely approximating posterior PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "lrtkIQZXwlMZ",
    "outputId": "86de88d2-193e-411a-ea15-937400ca5c2d"
   },
   "outputs": [],
   "source": [
    "RMSEs = np.array(\n",
    "    [\n",
    "        qp.metrics.quick_rmse(p, q, N=granularity)\n",
    "        for p, q in zip(posteriors_test, approx_pdf_on_grid)\n",
    "    ]\n",
    ")\n",
    "RMSEs_bias = np.array(\n",
    "    [\n",
    "        qp.metrics.quick_rmse(p, q, N=granularity)\n",
    "        for p, q in zip(posteriors_test, approx_pdf_on_grid_bias)\n",
    "    ]\n",
    ")\n",
    "\n",
    "plt.hist(np.log(RMSEs), alpha=0.5, bins=100, label=\"representative\", density=True)\n",
    "plt.hist(np.log(RMSEs_bias), alpha=0.5, bins=100, label=\"biased\", density=True)\n",
    "plt.xlabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTwhvAxUxVnf"
   },
   "source": [
    "### Your turn!\n",
    "\n",
    "How else can we quantify the performance of estimators of aleatoric uncertainty?\n",
    "\n",
    "**CHALLENGE**: Apply and visualize the local metrics of [Zhao, Dalmasso, Izbicki & Lee, 2021](https://arxiv.org/abs/2102.10473), or any other metrics not included in this demo; the [cde-diagnostics tutorial](https://github.com/zhao-david/CDE-diagnostics/blob/main/tutorial/tutorial-cde-diagnostics.ipynb), written by David Zhao (CMU), may be a good starting point.\n",
    "\n",
    "**CHALLENGE**: Use the differences between estimated and true photo-$z$ posterior PDFs as a function of photometry to isolate the implicit prior of epistemic uncertainty from the aleatoric uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3lapDjSqtbx"
   },
   "source": [
    "# Your turn: How to participate in this challenge tl;dr\n",
    "\n",
    "The organizers have identified three hack-able aspects to this data challenge:\n",
    "- Quantify sensitivity of ML photo-z posterior estimators to training set non-representativity\n",
    "- Implement and test additional AI methods for photo-z posterior estimation\n",
    "- Implement/interpret additional metrics of posterior precision\n",
    "\n",
    "Of course, there are many other possibilities for what to investigate based on this starting material!\n",
    "\n",
    "To participate, clone this tutorial's [GitHub repo](https://github.com/aimalz/qtc2021), make or comment on [an issue](https://github.com/aimalz/qtc2021/issues) saying what you'd like to hack on, and work in a branch corresponding to that issue.\n",
    "The only rule is that if your hack leads to a publication, you cite the repo (and, of course, invite those who contributed to your solution to be authors)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lZsWWSsss9gd"
   ],
   "name": "data challenge prototype",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
